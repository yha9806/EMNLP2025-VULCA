# MLLM Chinese Painting Critique Ability Assessment Experiment Manual

## 1. Overview

This document is the operational manual for the Multimodal Large Language Model (MLLM) assessment phase within the project "Comparative Analysis of Chinese Painting Critique Abilities: Human Scholars vs. Large Language Models." The core objective of this phase is to systematically evaluate the capabilities of baseline open-source MLLMs and MLLMs enhanced with "Persona Cards" and a knowledge base in generating Chinese painting critiques.

This manual provides detailed guidance for users to complete "Phase 2: Baseline MLLM Evaluation" and "Phase 3: Persona-Card Enhanced MLLM Evaluation" as defined in the main project `experiment/README.md`. For research background, methodology, and final findings, please refer to the project research paper `docs/main_paper.tex`.

## 2. Prerequisites

Before starting this experimental phase, please ensure the following conditions are met:

*   **Completion of Phase 1:** The human scholar benchmark construction must be completed, with all related deliverables (e.g., feature extraction scripts, analysis results, benchmark persona definitions, semantic space vectors) located as expected in the `experiment/human_expert/` directory.
*   **Datasets:**
    *   **Lang Shining's "Twelve Months" Paintings - Image Slices and Annotations:** The painting image slices (640x640 pixels) generated by the `data/noAI1.5.py` script and their detailed annotation data (visual elements, cultural symbols, artistic techniques, etc.) used for generating critiques. This data should be prepared beforehand. Experiments will be conducted separately for slices derived from 'large', 'medium', and 'small' original sampling windows. For example, an individual image slice file path would be `data/images/proceed/清_清院_十二月令图_一月_正月/large/清_清院_十二月令图_一月_正月_slice_2560_0_0.png`, where 'large' indicates the original sampling window size before the slice was resized to 640x640. Corresponding annotation files for the paintings should also be available.
    *   **Knowledge Base File:** `experiment/MLLMS/knowledge_dataset/knowledge_base.json` must exist and its content be complete.
*   **Environment Configuration:**
    *   Python 3.x
    *   Hugging Face Transformers
    *   Sentence Transformers
    *   PyTorch
    *   Pandas
    *   (Recommended) Install all necessary Python packages according to `experiment/human_expert/README.md` or the project root's `requirements.txt`.
*   **MLLM Model Access:**
    *   Ensure access to the MLLM models mentioned in the research paper, for example:
        *   `Qwen/Qwen2.5-VL-7B-Instruct`
        *   `Qwen/Qwen2.5-Omni-7B`
        *   `deepseek-ai/Janus-Pro-7B`
        *   `deepseek-ai/deepseek-vl2`
        *   `meta-llama/Llama-4-Scout-17B-16E-Instruct`
        *   `Qwen/Qwen3-235B-A22B`
        *   `google/gemma-3-27b-it`
    *   Access methods may include local deployment or via API. If using the Hugging Face API, you will need to obtain your own API key from https://huggingface.co/settings/tokens. **Please note: API keys are sensitive information. Keep them confidential and do not hardcode them directly into scripts or share them publicly. It is recommended to manage them using environment variables or a secure configuration file.**

## 3. Suggested Directory Structure for `experiment/MLLMS/`

To clearly manage the assets and outputs of this phase, it is recommended to organize files within the `experiment/MLLMS/` directory as follows (some paths have been adjusted according to your actual structure):

```
experiment/
└── MLLMS/
    ├── README.md                   <-- This operational manual (English version)
    ├── knowledge_dataset/
    │   └── knowledge_base.json     <-- Domain knowledge base
    ├── persona/                    <-- Persona card directory
    │   ├── 郭熙_Guo_Xi.md          <-- Persona Card: Guo Xi (Actual example)
    │   ├── 苏轼_Su_Shi.md          <-- Persona Card: Su Shi (Actual example)
    │   ├── ...                     <-- Other actual persona cards (8 .md files in total)
    │   └── README.md               <-- (Optional) Persona card descriptions
    ├── prompt/
    │   └── prompt.md               <-- Main prompt template (requires script adaptation)
    ├── src/
    │   ├── generate_mllm_critiques.py <-- Main critique generation script
    │   ├── vectorize_comments.py     <-- (To be created) Critique text vectorization script
    │   └── utils.py                  <-- (Optional) Utility functions module
    └── results/                      <-- Results directory (ensure plural form)
        ├── baseline_comments/          <-- Output: Critiques generated by baseline MLLMs
        │   └── qwen_vl_7b/
        │       └── twelve_months_january.txt (Example)
        ├── persona_comments/           <-- Output: Critiques generated by persona-enhanced MLLMs
        │   └── qwen_vl_7b_Guo_Xi/
        │       └── twelve_months_january.txt (Example)
        ├── baseline_vectors/           <-- Output: Vectors for baseline critiques
        └── persona_vectors/            <-- Output: Vectors for enhanced critiques
```
**Note:** The `results/` directory and its subdirectory structure are recommended best practices. If your `result/` (singular) directory has not yet been adjusted, please consider organizing it according to this structure.

## 4. Phase 2: Baseline MLLM Evaluation - Step-by-Step Guide

This phase aims to obtain baseline critique performance from each selected MLLM without specific persona guidance.

1.  **Test Case Preparation:**
    *   Confirm the specific paintings from Lang Shining's "Twelve Months" series to be used for critique generation (e.g., select January, May, August paintings as representatives).
    *   Ensure 640x640 pixel image slices (derived from 'large', 'medium', and 'small' sampling windows via `data/noAI1.5.py`) and detailed annotations for each selected painting are ready. The experiment will be run in batches, one for each original sampling window size (large, medium, small), using different image slices from the selected paintings for each batch.

2.  **Model Configuration and Loading:**
    *   In the `src/generate_mllm_critiques.py` script, configure the logic for loading the selected MLLMs.
    *   Ensure correct handling of image input (if the model supports it) and text prompts.

3.  **Prompt Engineering:**
    *   Primarily use the `prompt/prompt.md` file as the prompt template.
    *   When executing the baseline evaluation, the `src/generate_mllm_critiques.py` script needs to use or adapt the content of this `prompt.md` into a neutral, structured prompt that does not contain specific persona instructions. This prompt should guide the model to generate critiques in the format described in the paper (e.g., paragraph-style analysis + structured evaluation table).
    *   The script should be able to read this template and populate specific information for each painting (e.g., painting name, core observation points).

4.  **Comment Generation:**
    *   Execute the `src/generate_mllm_critiques.py` script.
    *   **Invocation Method:** This script needs to support a "baseline mode" or be configurable via parameters to generate critiques without persona enhancement. For example, when processing a slice derived from a 'large' sampling window:
        `python experiment/MLLMS/src/generate_mllm_critiques.py --mode baseline --model Qwen/Qwen2.5-VL-7B-Instruct --painting_id twelve_months_january_slice_large_2560_0_0 --image_path data/images/proceed/清_清院_十二月令图_一月_正月/large/清_清院_十二月令图_一月_正月_slice_2560_0_0.png --output_dir experiment/MLLMS/results/baseline_comments/`
    *   The script should iterate through the selected paintings (their respective slices for the current batch's original sampling size) and configured MLLM models.
    *   Generated critique texts should be saved to a path reflecting the model and original slice size, e.g., `results/baseline_comments/<model_name>/<slice_origin_size>/<painting_id_with_slice_info>.txt`.

5.  **Comment Vectorization:**
    *   **(To be created script)** Create and execute the `src/vectorize_comments.py` script.
    *   This script should read all critique texts generated in `results/baseline_comments/`.
    *   Use the `BAAI/bge-large-zh-v1.5` model specified in the paper to convert texts into 1024-dimension vectors.
    *   Generated vectors should be saved to the `results/baseline_vectors/` directory, preferably in `.pt` (PyTorch tensor) or `.npy` (NumPy array) format, maintaining a clear correspondence with the original critique files.

## 5. Phase 3: Persona-Card Enhanced MLLM Evaluation - Step-by-Step Guide

This phase aims to evaluate the impact of persona cards and the knowledge base on MLLM critique generation capabilities.

1.  **Persona Card Preparation:**
    *   Ensure the `persona/` directory contains all eight `.md` format persona card definition files (e.g., `郭熙_Guo_Xi.md`). The `src/generate_mllm_critiques.py` script needs to be able to read and parse these files to obtain persona descriptions.

2.  **Knowledge Base Application:**
    *   The `experiment/MLLMS/knowledge_dataset/knowledge_base.json` file must be accessible by the script.
    *   **Application Method:** When executing the persona-card enhanced evaluation, the `src/generate_mllm_critiques.py` script needs to implement the "[Specify application method]" mentioned in the paper to integrate the knowledge base.
        *   **If the paper does not specify a method, it is recommended to implement:** Before generating the prompt, dynamically retrieve the 1-2 most relevant knowledge snippets from `knowledge_base.json` based on the current persona card (e.g., `郭熙_Guo_Xi.md`) and the painting's theme.
        *   These knowledge snippets are then integrated with the persona description into the prompt template adapted from `prompt/prompt.md`.
        *   **Important Note:** The `src/generate_mllm_critiques.py` script must explicitly include this logic for knowledge base retrieval and prompt injection.

3.  **Model Configuration and Loading:**
    *   Same as in Phase 2, configure in `src/generate_mllm_critiques.py`.

4.  **Prompt Engineering:**
    *   Continue to use `prompt/prompt.md` as the base template.
    *   The `src/generate_mllm_critiques.py` script needs to dynamically integrate the persona description read from `persona/`, relevant knowledge retrieved from `knowledge_base.json`, and specific information about the painting to form the final, enhanced structured prompt.

5.  **Comment Generation:**
    *   Execute the `src/generate_mllm_critiques.py` script.
    *   **Invocation Method:** This script needs to support a "persona-enhanced mode." For example, when processing a slice derived from a 'large' sampling window with the Guo Xi persona:
        `python experiment/MLLMS/src/generate_mllm_critiques.py --mode persona --model Qwen/Qwen2.5-VL-7B-Instruct --persona_file experiment/MLLMS/persona/郭熙_Guo_Xi.md --painting_id twelve_months_january_slice_large_2560_0_0 --image_path data/images/proceed/清_清院_十二月令图_一月_正月/large/清_清院_十二月令图_一月_正月_slice_2560_0_0.png --output_dir experiment/MLLMS/results/persona_comments/ --knowledge_base experiment/MLLMS/knowledge_dataset/knowledge_base.json`
    *   The script should iterate through the selected paintings (their respective slices for the current batch's original sampling size), configured MLLM models, and each persona card.
    *   Generated critique texts should be saved to a path reflecting the model, persona, and original slice size, e.g., `results/persona_comments/<model_name>_<persona_name_from_file>/<slice_origin_size>/<painting_id_with_slice_info>.txt`.

6.  **Comment Vectorization:**
    *   **(To be created script)** Execute (or adapt) the `src/vectorize_comments.py` script again.
    *   Process all newly generated critique texts in `results/persona_comments/`.
    *   Save the generated vectors to the `results/persona_vectors/` directory.

## 6. Data Analysis and Evaluation - Overview

After completing the critique generation and vectorization described above, the produced data will be used for in-depth comparative analysis:

*   **Vector Space Analysis:**
    *   Compare D1 (human expert benchmark vectors) with D2 (baseline MLLM critique vectors from `results/baseline_vectors/`).
    *   Compare $D_{	ext{persona+KB}}$ (persona-card and knowledge base enhanced MLLM critique vectors from `results/persona_vectors/`) with D2.
    *   Compare $D_{	ext{persona+KB}}$ with D1.
    *   Conduct cross-persona card comparisons.
    *   Analysis methods include cosine similarity, Earth Mover's Distance (EMD) calculation, t-SNE/UMAP dimensionality reduction visualization, etc. (Refer to Sections 3 and 4 of the paper).
*   **Multi-Dimensional Capability Assessment:**
    *   Based on the core evaluation dimensions defined in the paper (evaluative stance, feature coverage and analysis depth, argumentative quality, textual professionalism, semantic similarity) and the evaluation criteria in the appendix (Appendix 
ef{app:evaluation}), score and analyze the generated critique texts.
    *   Structured critique outputs (e.g., including self-assessment tables) can be directly used for this evaluation.

These detailed analysis steps and scripts may be located in a higher-level analysis directory (e.g., `analysis_scripts/` or `experiment/analysis/` in the project root), or as additional scripts under this `experiment/MLLMS/src/` (e.g., `analyze_mllm_performance.py`).

## 7. Expected Outputs from MLLMS Experimental Phase

Upon completion of the experimental phase guided by this manual, the following main outputs should be obtained:

*   **Baseline Critique Texts:** Stored in `results/baseline_comments/`, containing critiques generated by various baseline MLLMs for the test painting image slices (grouped by original sampling window size: large, medium, small).
*   **Enhanced Critique Texts:** Stored in `results/persona_comments/`, containing critiques generated by various MLLMs under different persona card and knowledge base enhancement conditions, for the test painting image slices (grouped by original sampling window size: large, medium, small).
*   **Critique Vectors:** Stored in `results/baseline_vectors/` and `results/persona_vectors/`, representing the 1024-dimension semantic vector representations for the two types of critique texts mentioned above, corresponding to the different image slice inputs.
*   **(Optional) Preliminary Evaluation Data:** If preliminary automated evaluation metrics (e.g., rule-based checks, specific keyword frequencies) are integrated into the scripts, the relevant data should also be saved.

These outputs will serve as the core data foundation for subsequent comprehensive analysis and paper writing for the project.

---
**Note:** This manual provides the framework and guidance for conducting the experiments. The specific script implementations (`.py` files), particularly the mode-switching logic and knowledge base integration for `src/generate_mllm_critiques.py`, and the to-be-created `src/vectorize_comments.py`, need to be designed and written in detail by the user based on the descriptions above, the overall project objectives, and the APIs of the libraries used. 