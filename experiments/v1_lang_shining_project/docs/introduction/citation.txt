1. Kessler, S. H., Mahl, D., Schäfer, M. S. and Volk, S. C. (2025). Science Communication in the Age of Artificial Intelligence JCOM 24(2), E. https://doi.org/10.22323/2.24020501
Science Communication in the Age of Artificial Intelligence
Sabrina Heike Kessler
,
Daniela Mahl
,
Mike S. Schäfer
,
Sophia C. Volk
Artificial Intelligence (AI) is fundamentally transforming science communication. This editorial for the JCOM Special Issue “Science Communication in the Age of AI” explores the implications of AI, especially generative AI, for science communication, its promises and challenges. The articles in this Special Issue can be categorized into four key areas: (1) communication about AI, (2) communication with AI, (3) the impact of AI on science communication ecosystems, and (4) AI’s influence on science, theoretical and methodological approaches. This collection of articles advances empirical and theoretical insight into AI’s evolving role in science communication, emphasizing interdisciplinary and comparative perspectives.
1 Introduction
Artificial intelligence (AI), and particularly generative AI (GenAI), is fundamentally transforming science communication, from how science-related content is produced over how it is disseminated all the way to public engagement with science [Biyela et al., 2024]. With large language models like GPT, Gemini, or Mistral, AI now generates text, images, and audiovisual content, disrupting traditional media and communication ecosystems and impacting the communication of science. While GenAI can enhance accessibility, interactivity, and efficiency in explaining complex topics through its dialogical potential, it also raises considerable concerns about transparency, biases, and dis- and misinformation [Gravel et al., 2023; Volk et al., 2024]. As AI becomes a key intermediary for science-related information [Greussing, Guenther, Baram-Tsabari, Dabran-Zivan, Jonas, Klein-Avraham, Taddicken, Agergaard, Beets, Brossard, Chakraborty et al., 2025; Fletcher & Nielsen, 2024], understanding its strengths and limitations is critical for the future of science communication — and for research in this field [Schäfer, 2023].
Kessler, S. H., Mahl, D., Schäfer, M. S. and Volk, S. C. (2025). Science Communication in the Age of Artificial Intelligence JCOM 24(2), E. https://doi.org/10.22323/2.24020501

2. Use of large language models as artificial intelligence tools in academic research and publishing among global clinical researchers
Abstract
With breakthroughs in Natural Language Processing and Artificial Intelligence (AI), the usage of Large Language Models (LLMs) in academic research has increased tremendously. Models such as Generative Pre-trained Transformer (GPT) are used by researchers in literature review, abstract screening, and manuscript drafting. However, these models also present the attendant challenge of providing ethically questionable scientific information. Our study provides a snapshot of global researchers’ perception of current trends and future impacts of LLMs in research. Using a cross-sectional design, we surveyed 226 medical and paramedical researchers from 59 countries across 65 specialties, trained in the Global Clinical Scholars’ Research Training certificate program of Harvard Medical School between 2020 and 2024. Majority (57.5%) of these participants practiced in an academic setting with a median of 7 (2,18) PubMed Indexed published articles. 198 respondents (87.6%) were aware of LLMs and those who were aware had higher number of publications (p < 0.001). 18.7% of the respondents who were aware (n = 37) had previously used LLMs in publications especially for grammatical errors and formatting (64.9%); however, most (40.5%) did not acknowledge its use in their papers. 50.8% of aware respondents (n = 95) predicted an overall positive future impact of LLMs while 32.6% were unsure of its scope. 52% of aware respondents (n = 102) believed that LLMs would have a major impact in areas such as grammatical errors and formatting (66.3%), revision and editing (57.2%), writing (57.2%) and literature review (54.2%). 58.1% of aware respondents were opined that journals should allow for use of AI in research and 78.3% believed that regulations should be put in place to avoid its abuse. Seeing the perception of researchers towards LLMs and the significant association between awareness of LLMs and number of published works, we emphasize the importance of developing comprehensive guidelines and ethical framework to govern the use of AI in academic research and address the current challenges.

Similar content being viewed by others

The future landscape of large language models in medicine
Article Open access
10 October 2023

The imperative for regulatory oversight of large language models (or generative AI) in healthcare
Article Open access
06 July 2023

Evaluation and mitigation of the limitations of large language models in clinical decision-making
Article Open access
04 July 2024
Introduction
Large Language Models (LLMs) represent a significant breakthrough in Natural Language Processing (NLP) and Artificial Intelligence (AI)1. Prior to 2017, while NLP models could perform several language processing tasks, they were not easily accessible to non-domain experts. The introduction of the Transformer architecture in 2017 revolutionized the field, enabling NLP models to efficiently synthesize and analyze datasets using simple prompts. This allowed large-scale use by people worldwide, significantly broadening access to advanced language processing tools2. The Transformer technology led to the development of two game changers: Bidirectional Encoder Representations from Transformers (BERT) and Generative Pretrained Transformer (GPT), that used semi-supervised approach and acquired exceptional generalization capabilities with the ability to interpret and generate human-like text3. The launch of ChatGPT in 2022 gained public attention in almost every field of life owing to its accessibility and user-friendly interface. LLMs offer AI driven support particularly in literature review, summarizing articles, abstract screening, extracting data and drafting manuscript. Due to workload reduction and the ease offered, there has been an increasing interest in the incorporation of LLMs like ChatGPT, Perplexity, Llama by Meta (formerly Facebook), Google Bard and Claude, in academic research, as indicated by a rapid increase in the number of articles after ChatGPT’s release3,4.

Although there are numerous efficiency gains in utilizing LLMs in research, they however cannot replace humans particularly in contexts where meticulous understanding and original thought, along with accountability are crucial5,6. With deeper understanding of LLMs, it was found that LLMs are also capable of generating fake citations, rapidly generating large volumes of questionable information, and also amplifying biases3,7. This has led to negative ethical implications like authorship integrity and a surge in predatory practices and as a consequence, an “AI-driven infodemic” has emerged5. There is also a risk of public health threat resulting from ghost-written scientific articles, fake news and misinforming content3. In addressing these issues, as a first step, it is pertinent to understand attitudes of researchers towards LLMs in research by assessing researcher’s awareness and practices of the use of LLMs.

Our study provides a unique analysis of a targeted group of medical and paramedical researchers enrolled in a one-year-certification course- Global Clinical Scholars Research Training (GCSRT) Program, at Harvard Medical School (HMS). We aim to provide insights into the current trends in AI usage in research and publication along with a peek into the future scope and impact of LLMs. We strongly believe that the results of our study can aid journals to formulate future policies regarding the use of AI tools in the process of publication, thus ensuring credibility and maintaining integrity of medical publications.

Methods
Study design and population
This global survey was carried out using a cross-sectional design. It was conducted between April and June 2024, amongst a diverse group of medical and paramedical researchers who received training at the GCSRT program at Harvard. This program consists of researchers from over 50 countries and 6 continents spanning various specialties, career stages, age groups and genders. At the program, all participants receive advanced training in every stage of research including statistical analysis, publishing and grant writing8. They are therefore an ideal group to assess AI tools usage in research.

Study objectives
We had three primary objectives for this study. First, to assess the level of awareness of LLMs amongst global researchers. Second, to identify how LLMs are currently used in academic research and publishing amongst our survey respondents. And third, to analyze the potential future impact and ethical implications of AI tools in medical research and publishing.

Eligibility criteria
(a)
Inclusion Criteria: Medical and paramedical researchers who have been participants of the GCSRT program at HMS belonging to any cohort between 2020 and 2024, irrespective of their country of origin, research interests, active years in research, age or gender. Researchers who were members of the unofficial class WhatsApp groups and were proficient in reading and writing in English language were included specifically.

(b)
Exclusion Criteria: Researchers from cohorts outside of the above specified years, those who were not accessible through class WhatsApp groups, or were not proficient in reading and writing in English language were excluded from the study. Medical and paramedical researchers who have not undergone training at this program as well as non-medical researchers, were not invited for this study.

Questionnaire development and survey dissemination strategy
The survey was drafted using Google Forms, in English Language. It consisted of a total of 4 sections to cover our primary objectives- (1) Background, (2) Awareness of LLMs, (3) Impact of LLMs and (4) Future Policy. Each question was carefully reviewed for its relevance, validity, and unbiasedness. Data collectors for the study were voluntarily chosen from amongst the participants of the GCSRT Program. The data collectors from each of the targeted cohorts were made primary in-charge of reaching out to our target population in their cohort via personal messaging on WhatsApp and LinkedIn. The contact information of the survey respondents was obtained from the unofficial class WhatsApp groups and personal networks of the data collectors. A total of 3 personal messages including 2 reminders, spaced 7 days apart each, were sent to each prospective participant. Informed consent was obtained, and Google survey forms were filled out by a total of 226 researchers from over 59 countries.

Sample size and statistical methods
The link to the Google survey form was distributed to 5 cohorts of the GCSRT program consisting of a total of 550 medical and paramedical researchers. A total sample size of 220 was calculated by considering a margin of error of 5%, a confidence level of 95% and power of 0.8. Descriptive statistics of the survey respondents were presented as mean ± standard deviation for normally distributed continuous data, median (interquartile range) for non-normally distributed continuous data, and frequencies & percentages for categorical data. Continuous data were tested for normality using the Shapiro–Wilk test. Normally distributed data were analyzed using one way ANOVA while non-normally distributed data were analyzed using the Kruskal-Wallis test. Categorical data were analyzed with Chi-squared test or Fisher’s exact test. Qualitative data from open-ended questions were studied via thematic analysis. All statistical analyses were performed in Stata MP version 17.0 (StataCorp, College Station, TX, USA). All tests were two-tailed and considered significant at P < 0.05.

Ethical consideration
In accordance with the declaration of Helsinki8, this study was approved by the ethical review board at Allama Iqbal Medical College/ Jinnah Hospital Lahore, Pakistan (Reference no: ERB 163/9/30-04-2024/S1 ERB). It is not supported or endorsed by HMS. However, timely notification about the study was provided to the administration of the GCSRT program. Consent to participate was collected from every respondent as the first, mandatory response to the questionnaire. All personal information like email ID, nationality, and age was carefully de-identified and handled confidentially. The respondents were provided necessary information on the voluntariness of the study as well as contact information of the principal investigator.
Mishra, T., Sutanto, E., Rossanti, R. et al. Use of large language models as artificial intelligence tools in academic research and publishing among global clinical researchers. Sci Rep 14, 31672 (2024). https://doi.org/10.1038/s41598-024-81370-6

citation:
TY  - JOUR
AU  - Mishra, Tanisha
AU  - Sutanto, Edward
AU  - Rossanti, Rini
AU  - Pant, Nayana
AU  - Ashraf, Anum
AU  - Raut, Akshay
AU  - Uwabareze, Germaine
AU  - Oluwatomiwa, Ajayi
AU  - Zeeshan, Bushra
PY  - 2024
DA  - 2024/12/30
TI  - Use of large language models as artificial intelligence tools in academic research and publishing among global clinical researchers
JO  - Scientific Reports
SP  - 31672
VL  - 14
IS  - 1
AB  - With breakthroughs in Natural Language Processing and Artificial Intelligence (AI), the usage of Large Language Models (LLMs) in academic research has increased tremendously. Models such as Generative Pre-trained Transformer (GPT) are used by researchers in literature review, abstract screening, and manuscript drafting. However, these models also present the attendant challenge of providing ethically questionable scientific information. Our study provides a snapshot of global researchers’ perception of current trends and future impacts of LLMs in research. Using a cross-sectional design, we surveyed 226 medical and paramedical researchers from 59 countries across 65 specialties, trained in the Global Clinical Scholars’ Research Training certificate program of Harvard Medical School between 2020 and 2024. Majority (57.5%) of these participants practiced in an academic setting with a median of 7 (2,18) PubMed Indexed published articles. 198 respondents (87.6%) were aware of LLMs and those who were aware had higher number of publications (p < 0.001). 18.7% of the respondents who were aware (n = 37) had previously used LLMs in publications especially for grammatical errors and formatting (64.9%); however, most (40.5%) did not acknowledge its use in their papers. 50.8% of aware respondents (n = 95) predicted an overall positive future impact of LLMs while 32.6% were unsure of its scope. 52% of aware respondents (n = 102) believed that LLMs would have a major impact in areas such as grammatical errors and formatting (66.3%), revision and editing (57.2%), writing (57.2%) and literature review (54.2%). 58.1% of aware respondents were opined that journals should allow for use of AI in research and 78.3% believed that regulations should be put in place to avoid its abuse. Seeing the perception of researchers towards LLMs and the significant association between awareness of LLMs and number of published works, we emphasize the importance of developing comprehensive guidelines and ethical framework to govern the use of AI in academic research and address the current challenges.
SN  - 2045-2322
UR  - https://doi.org/10.1038/s41598-024-81370-6
DO  - 10.1038/s41598-024-81370-6
ID  - Mishra2024
ER  - 

3. Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers
Abstract
Large language models such as ChatGPT can produce increasingly realistic text, with unknown information on the accuracy and integrity of using these models in scientific writing. We gathered fifth research abstracts from five high-impact factor medical journals and asked ChatGPT to generate research abstracts based on their titles and journals. Most generated abstracts were detected using an AI output detector, ‘GPT-2 Output Detector’, with % ‘fake’ scores (higher meaning more likely to be generated) of median [interquartile range] of 99.98% ‘fake’ [12.73%, 99.98%] compared with median 0.02% [IQR 0.02%, 0.09%] for the original abstracts. The AUROC of the AI output detector was 0.94. Generated abstracts scored lower than original abstracts when run through a plagiarism detector website and iThenticate (higher scores meaning more matching text found). When given a mixture of original and general abstracts, blinded human reviewers correctly identified 68% of generated abstracts as being generated by ChatGPT, but incorrectly identified 14% of original abstracts as being generated. Reviewers indicated that it was surprisingly difficult to differentiate between the two, though abstracts they suspected were generated were vaguer and more formulaic. ChatGPT writes believable scientific abstracts, though with completely generated data. Depending on publisher-specific guidelines, AI output detectors may serve as an editorial tool to help maintain scientific standards. The boundaries of ethical and acceptable use of large language models to help scientific writing are still being discussed, and different journals and conferences are adopting varying policies.

Similar content being viewed by others

Identification of dental related ChatGPT generated abstracts by senior and young academicians versus artificial intelligence detectors and a similarity detector
Article Open access
02 April 2025

Prompt engineering in ChatGPT for literature review: practical guide exemplified with studies on white phosphors
Article Open access
01 May 2025

Detection of ChatGPT fake science with the xFakeSci learning algorithm
Article Open access
14 July 2024
The release of OpenAI’s free tool ChatGPT1 on November 30, 2022 demonstrated the ability of artificial intelligence models to generate content, with articles quickly published on its possible uses and potential controversies2,3,4. Early adopters have shared their experiences on social media, with largely positive sentiments5. Articles are bemoaning the death of the traditional school essay assignment4,6,7, as ChatGPT has been shown to generate high-scoring papers8, correctly answer USMLE questions9, and even articulate critical thinking10. The ethical and acceptable boundaries of ChatGPT’s use in scientific writing remain unclear11, although some publishers are beginning to lay down policies12,13,14.

Large language models (LLM) are often complex neural network-based transformer models that can generate tone and content-defined text. These are trained on enormous amounts of data to predict the best next text element, which produces a product that reads naturally. ChatGPT is built on Generative Pre-trained Transformer-3 (GPT-3), which is one of the largest of these types of models, trained with 175 billion parameters15. These models generate coherent and fluent output, that can be difficult to distinguish from text written by humans16,17.

Artificial intelligence (AI) has numerous applications in medical technologies18, and the writing of biomedical research is no exception, with products such as the SciNote Manuscript Writer19 or Writefull20 that help with scientific writing. However, with the release of ChatGPT, this powerful LLM technology is now available to all users for free, and millions are engaging with the new technology. The user base is likely to continue to grow. Thus, there is an urgent need to determine if ChatGPT can write convincing medical research abstracts.

We gathered 50 abstracts from five high-impact journals as our control corpus of well-written abstracts. We asked ChatGPT to generate 50 scientific abstracts based on the titles and specific journals from this list (example subset in Supplementary Data 1). While all the output appeared superficially to be formatted as a scientific abstract, only 8 (16%) correctly used the headings particular to the specific journal in the prompt (e.g., Nature Medicine’s paragraph-style without headings, as opposed to specific headings such as ‘Design, Setting, and Participants’ for JAMA, see Supplementary Note 1 for examples). The patient cohort sizes were a similar order of magnitude between the original abstracts and the generated abstracts, with a Pearson correlation of the logarithmic cohort sizes of r = 0.76, p < 0.001 (Fig. 1).

Fig. 1: Generated abstracts have a similar patient cohort size as original abstracts.
figure 1
Cohort sizes from original abstracts (x-axis) and generated abstracts (y-axis) plotted on a logarithmic 10 scale.

Full size image
The AI output detector we used, ‘GPT-2 Output Detector’21,22, found a high probability of AI-generated output (higher % ‘fake’ score indicating more likely to be AI-generated text) in the generated abstracts with median [IQR] of 99.98% [12.73%, 99.98%] compared with very low probability of AI-generated output in nearly all the original abstracts with median [IQR] of 0.02% [0.02%, 0.09%] (Fig. 2a). The AI output detector had an area under the receiver operating characteristics (AUROC) curve of 0.94 for detecting generated abstracts (Fig. 2b). At the optimal cutoff maximizing sensitivity and specificity, 1.46%, the AI output detector had a sensitivity of 86% and a specificity of 94% at differentiating original versus generated abstracts.

Fig. 2: Many generated abstracts can be detected using an AI output detector.
figure 2
a AI detection scores as [% ‘fake’] per GPT-2 Output Detector for original abstracts and generated abstracts. Higher score indicates more likely to be generated by AI. b The AI output detector ROC curve for discriminating between original and generated abstracts, with AUROC of 0.94.

Full size image
We ran both original and generated abstracts through a free plagiarism-detection website, Plagiarism Detector20, and a paid professional similarity checker, iThenticate23. For both platforms, a higher score indicates more matching text was found. Original abstracts scored higher on the Plagiarism Detector website with median ‘plagiarized’ score 62.5% [IQR 43.25%, 84.75%] compared with generated abstracts with median ‘plagiarized’ score of 0% [IQR 0, 0] (Fig. 3a). Original abstracts also scored higher on iThenticate with median similar index of 100 [IQR 100, 100] compared with generated abstracts that had with median similarity index of 27 [IQR 19, 40.75] (Fig. 3b).

Fig. 3: Generated abstracts score lower than original abstracts on plagiarism detectors.
figure 3
a Plagiarism scores from plagiarism detector website, with higher % ‘plagiarized’ score indicating more matching text was found. b iThenticate Similarity Index for original abstracts and generated abstracts [%], with higher value meaning more similar text was found.

Full size image
Blinded human reviewers (FMH, NSM, ECD, SR) were given a mixture of real and generated abstracts and asked to provide a binary score of real or generated (Table 1). They were able to correctly identify 68% of generated abstracts as being generated, and correctly identified 86% of original articles as being original. They incorrectly identified 32% of generated abstracts as being real, and 14% of original abstracts as being generated. Our reviewers commented that abstracts they thought were generated by ChatGPT were superficial and vague, and sometimes focused on details of original abstracts such as inclusion of Clinical Trial Registration numbers and alternative spellings of words. The AI output detector scores were not statistically different (p = 0.45 by MWW) between the abstracts that reviewers correctly identified as generated and ones that they failed to identify as generated (Fig. 4).

Table 1 Human reviewer scoring for whether abstracts were real or generated, along with truth.
Full size table
Fig. 4: Reviewers use criteria different than the AI output detector for flagging abstracts as either generated or original.
figure 4
The AI detection scores for generated abstracts were not significantly different (p = 0.45) between abstracts that human reviewers identified as generated, and those that they failed to identify as generated.

Full size image
In this study, we found that both humans and AI output detectors were able to identify a portion of abstracts generated by ChatGPT, but neither were perfect discriminators. Our reviewers even misclassified a portion of real abstracts as being generated, indicating they were highly skeptical when reviewing the abstracts. The generated abstracts contained fabricated numbers but were in a similar range as the real abstracts; ChatGPT knew from its training data that studies on hypertension should include a much larger patient cohort size than studies on rarer diseases such as monkeypox.

Limitations to our study include its small sample size and few reviewers. ChatGPT is also known to be sensitive to small changes in prompts; we did not exhaust different prompt options, nor did we deviate from our prescribed prompt. ChatGPT generates a different response even to the same prompt multiple times, and we only evaluated one of infinite possible outputs. We took only the first output given by ChatGPT, without additional refinement that could enhance its believability or improve its escape from detection. Thus, our study likely underestimates the ability of ChatGPT to generate scientific abstracts. The maximum input for the AI output detector we used is 510 tokens, thus some of the abstracts were not able to be fully evaluated due to their length. Our study ream reviewers knew that a subset of the abstracts they were viewing were generated by ChatGPT, but a reviewer outside this context may not be able to recognize them as written by a large language model. We only asked for a binary response from our reviewer team of original or generated and did not use a formal or more sophisticated rubric. Future studies could expand on our methodology to include other AI output detector models, other plagiarism detectors, more formalized review, as well as text from other fields outside of biomedical sciences.

We anticipate that this technology could be used in both an ethical and unethical way. Given its ability to generate abstracts with believable numbers, it could be used by organizations such as paper mills to entirely falsify research. On the other hand, the technology may be used in conjunction with a researcher’s own scientific knowledge as a tool to decrease the burden of writing and formatting. It could be used by scientists publishing in a language that is not their native language, to improve equity. However, AI models have been shown to be highly sensitive to biases in training data24,25, and further data is needed to determine the potential for bias perpetuated by ChatGPT—especially given the overt prejudices emerging from prior language generation models26.

We suggest clear disclosure when a manuscript is written with assistance from large language models such as ChatGPT. Though there is ongoing work to embed watermarks in AI-generated output, it is unknown when this will be standardized and robust against scrubbing efforts. Reassuringly, there are patterns that allow it to be detected by AI output detectors, although there has been exploration of techniques to fool AI output detectors27. Different journals and publishers are developing their own policies on whether large language models can contribute to the writing of papers, ranging from not allowing any AI-generated text12 to allowing its use as long as it is openly disclosed13,14,28. Though imperfect, AI output detectors may be one tool to include in the research editorial process, depending on the publisher or conference’s guidelines.

Abstract generation by large language models such as ChatGPT is a powerful tool to create readable scientific abstracts, though includes generated data. The generated abstracts do not always alarm plagiarism-detection models, as the text is generated anew, but can often be detected using AI detection models, and sometimes identified by a person. Generated text may help alleviate the burden of writing by providing an outline for a scientist to edit but requires careful review for factual accuracy. The optimal use and ethical boundaries of AI-generated writing remain to be determined as discussion within the scientific community evolves.

Methods
Abstract generation
We evaluated the abstracts generated by ChatGPT (Version Dec 15) for 50 scientific medical papers. We gathered titles and original abstracts from current and recent issues (published in late November and December of 2022) of five high-impact journals (Nature Medicine, JAMA, NEJM, BMJ, Lancet) and compared them with the original abstracts. The prompt fed to the model was ‘Please write a scientific abstract for the article [title] in the style of [journal] at [link]’. Note that the link is superfluous because ChatGPT cannot browse the internet. ChatGPT’s knowledge cutoff date is September 2021. We ran each prompt in a new session.

Abstract evaluation
We evaluated the ChatGPT-generated abstracts for plagiarism detection using a free web-crawling plagiarism-detection tool ‘Plagiarism Detector’29, which gives a ‘plagiarized’ score from 0–100%, with a higher score indicating that more plagiarism was detected; these analyses were run in December. We also evaluated the abstracts using a paid similarity checker program, iThenticate23, which outputs a 0–100% ‘similarity index’, with a higher score indicating more redundant with existing text. We also evaluated abstracts with an AI output detector using the ‘GPT-2 Output Detector’21,22, a RoBERTa-based sequence classifier, which gives abstracts a score ranging from 0.02 to 99.98% ‘fake’, with a higher score indicating the text was more likely to be generated by an AI algorithm.

We evaluated whether blinded human reviewers (study team members FMH, NSM, ECD, SR, members of our biomedical sciences laboratories used to reading scientific abstracts) could identify ChatGPT-generated abstracts. For every pair of reviewers, we used randomization via an electronic coin flip to decide whether an original or generated abstract would be provided for the first reviewer, with the opposite being given to the second reviewer. Each reviewer was given 25 abstracts to review, informed that there was a mixture of original and generated abstracts, asked to give a binary score of whether they thought the abstract was original or generated and invited to make free-text observations while reviewing. Reviewers were not shown any data or analysis until after their scoring of abstracts was completed.

We gave a binary yes/no score of whether the format of the generated abstract adhered to the journal’s requirements by comparing it to the original article’s headings and structure. We also compared the reported patient cohort sizes between the original and generated abstracts with a Pearson correlation of the logarithmic cohort sizes.

Statistics and visualization
Graphics and statistics were done in Python version 3.9 with seaborn version 0.11.230, matplotlib version 3.5.131, sklearn version 1.0.232, scipy version 1.7.333, and statsannotations version 0.4.434. Group statistics are reported using median [interquartile range] and were compared using two-sided Mann Whitney Wilcoxon (MWW) tests, with p < 0.05 being the cutoff for statistical significance. Proportions were compared with Fisher’s Exact tests. Correlation between the cohort sizes was done with Pearson’s correlation.

Reporting summary
Further information on research design is available in the Nature Research Reporting Summary linked to this article.

citation:
Gao, C.A., Howard, F.M., Markov, N.S. et al. Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers. npj Digit. Med. 6, 75 (2023). https://doi.org/10.1038/s41746-023-00819-6

DOI
https://doi.org/10.1038/s41746-023-00819-6

4. The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training

Gi-Cheon Kang, Sungdong Kim, Jin-Hwa Kim, Donghyun Kwak, Byoung-Tak Zhang; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 6746-6756

Abstract

Visual dialog (VisDial) is a task of answering a sequence of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog agents solely on VisDial data via supervised learning or leveraged pre-training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for visually-grounded dialog, called Generative Self-Training (GST), to leverage unlabeled images on the Web. Specifically, GST first retrieves in-domain images through out-of-distribution detection and generates synthetic dialogs regarding the images via multimodal conditional text generation. GST then trains a dialog agent on the synthetic and the original VisDial data. As a result, GST scales the amount of training data up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For robust training of the synthetic dialogs, we also propose perplexity-based data selection and multimodal consistency regularization. Evaluation on VisDial v1.0 and v0.9 datasets shows that GST achieves new state-of-the-art results on both datasets. We further observe the robustness of GST against both visual and textual adversarial attacks. Finally, GST yields strong performance gains in the low-data regime. Code is available at https://github.com/gicheonkang/gst-visdial.

@InProceedings{Kang_2023_CVPR,
    author    = {Kang, Gi-Cheon and Kim, Sungdong and Kim, Jin-Hwa and Kwak, Donghyun and Zhang, Byoung-Tak},
    title     = {The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {6746-6756}
}

5. Evaluating the Factual Consistency of Large Language Models Through News Summarization
Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, Colin Raffel
While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at this https URL.
Subjects:	Computation and Language (cs.CL)
Cite as:	arXiv:2211.08412 [cs.CL]
 	(or arXiv:2211.08412v2 [cs.CL] for this version)
 
https://doi.org/10.48550/arXiv.2211.08412
Focus to learn more

6. BiasConnect: Investigating Bias Interactions in Text-to-Image Models
Pushkar Shukla1       Aditya Chinchure2       Emily Diana3       Alexander Tolbert4
      Kartik Hosanagar5       Vineeth N. Balasubramanian6       Leonid Sigal2       Matthew A. Turk1
1Toyota Technological Institute at Chicago       2University of British Columbia
3Carnegie Mellon University, Tepper School of Business       4Emory University
5University of Pennsylvania, The Wharton School       6Indian Institute of Technology Hyderabad
{pushkarshukla, mturk}@ttic.edu       {aditya10, lsigal}@cs.ubc.ca
Abstract
The biases exhibited by Text-to-Image (TTI) models are often treated as if they are independent, but in reality, they may be deeply interrelated. Addressing bias along one dimension, such as ethnicity or age, can inadvertently influence another dimension, like gender, either mitigating or exacerbating existing disparities. Understanding these interdependencies is crucial for designing fairer generative models, yet measuring such effects quantitatively remains a challenge. In this paper, we aim to address these questions by introducing BiasConnect, a novel tool designed to analyze and quantify bias interactions in TTI models. Our approach leverages a counterfactual-based framework to generate pairwise causal graphs that reveals the underlying structure of bias interactions for the given text prompt. Additionally, our method provides empirical estimates that indicate how other bias dimensions shift toward or away from an ideal distribution when a given bias is modified. Our estimates have a strong correlation (+0.69) with the interdependency observations post bias mitigation. We demonstrate the utility of BiasConnect for selecting optimal bias mitigation axes, comparing different TTI models on the dependencies they learn, and understanding the amplification of intersectional societal biases in TTI models.

Prabhu, V. U. et al. (2023). BiasConnect: Investigating Bias Interactions in Text-to-Image Models. arXiv:2503.09763

@article{Mishra2024,
    author    = {Mishra, Tanisha and Sutanto, Edward and Rossanti, Rini and Pant, Nayana and Ashraf, Anum and Raut, Akshay and Uwabareze, Germaine and Oluwatomiwa, Ajayi and Zeeshan, Bushra},
    title     = {Use of large language models as artificial intelligence tools in academic research and publishing among global clinical researchers},
    journal   = {Scientific Reports},
    volume    = {14},
    number    = {1},
    pages     = {31672},
    year      = {2024},
    month     = {Dec},
    doi       = {10.1038/s41598-024-81370-6},
    url       = {https://doi.org/10.1038/s41598-024-81370-6}
  }
  
  @article{Gao2023,
    author    = {Gao, Catherine A. and Howard, F. M. and Markov, N. S. and Dyer, E. C. and Ramesh, S. and Luo, Y. and Pearson, A. T.},
    title     = {Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers},
    journal   = {npj Digital Medicine},
    volume    = {6},
    number    = {1},
    pages     = {75},
    year      = {2023},
    doi       = {10.1038/s41746-023-00819-6},
    url       = {https://doi.org/10.1038/s41746-023-00819-6}
  }
  
  @misc{Shukla2023biasconnect,
    author       = {Shukla, Pushkar and Chinchure, Aditya and Diana, Emily and Tolbert, Alexander and Hosanagar, Kartik and Balasubramanian, Vineeth N. and Sigal, Leonid and Turk, Matthew A.},
    title        = {BiasConnect: Investigating Bias Interactions in Text-to-Image Models},
    year         = {2023},
    publisher    = {arXiv},
    doi          = {10.48550/ARXIV.2305.09763},
    url          = {https://arxiv.org/abs/2305.09763},
    eprintprefix = {arXiv},
    eprint       = {2305.09763}
  }
  
  @article{Kessler2025,
    author    = {Kessler, Sabrina Heike and Mahl, Daniela and Schäfer, Mike S. and Volk, Sophia C.},
    title     = {Science Communication in the Age of Artificial Intelligence},
    journal   = {JCOM Journal of Science Communication},
    volume    = {24},
    number    = {02},
    pages     = {E},
    year      = {2025},
    doi       = {10.22323/2.24020501},
    url       = {https://doi.org/10.22323/2.24020501}
  }
  
  @misc{Tam2022evaluating,
    author       = {Tam, Derek and Mascarenhas, Anisha and Zhang, Shiyue and Kwan, Sarah and Bansal, Mohit and Raffel, Colin},
    title        = {Evaluating the Factual Consistency of Large Language Models Through News Summarization},
    year         = {2022},
    publisher    = {arXiv},
    doi          = {10.48550/ARXIV.2211.08412},
    url          = {https://arxiv.org/abs/2211.08412},
    eprintprefix = {arXiv},
    eprint       = {2211.08412}
  }