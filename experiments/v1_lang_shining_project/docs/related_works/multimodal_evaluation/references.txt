# 多模态评估和文化理解 (Multimodal Evaluation and Cultural Understanding)

## 主要文献引用

1. Rott Shaham et al. (2024): A Multimodal Automated Interpretability Agent
提出 MAIA（Multimodal Automated Interpretability Agent），一种结合预训练视觉–语言模型与一套工具集，自动化地执行神经网络可解释性实验，包括合成／编辑输入、提取最大激活示例、归纳实验结果等。
在 neuron-level 特征解释和故障模式发现两方面，对视觉模型进行了评估：一方面生成的描述可与专家匹配，另一方面能辅助降低模型对"虚假"特征的敏感度并发现易错输入。
一个可解释性模型，和我们现在的研究没有什么关系？
@inproceedings{shaham2024multimodal,
    title={A multimodal automated interpretability agent},
    author={Rott Shaham, Tamar and Schwettmann, Sarah and Wang, Franklin and Rajaram, Achyuta and Hernandez, Evan and Andreas, Jacob and Torralba, Antonio},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024}
  }

(a) Huang & Zhang (2024): A Survey on Evaluation of Multimodal Large Language Models
系统性地综述了多模态大语言模型(MLLMs)的评估方法，涵盖了评估的背景、评估内容("what to evaluate")、评估基准("where to evaluate")和评估方法("how to evaluate")。文章按能力类型对现有MLLM评估任务进行了分类，包括一般多模态识别、感知、推理和可信度，以及特定领域应用。
@article{huang2024survey,
  title={A Survey on Evaluation of Multimodal Large Language Models},
  author={Huang, Jiaxing and Zhang, Jingyi},
  journal={arXiv preprint arXiv:2408.15769},
  year={2024}
}
（这个论文，可以用于introduction部分的内容）

(b) Xu et al. (2024): MultiSkill: Evaluating Large Multimodal Models for Fine-grained Alignment Skills
提出了MultiSkill评估协议，该协议评估大型多模态模型(LMMs)在多个细粒度技能上与人类价值观的一致性。MultiSkill将粗粒度评分分解为针对每个指令的细粒度技能集评分，定义了5个核心视觉-语言能力并细分为12个与用户指令一致所必需的技能。
@inproceedings{xu-etal-2024-multiskill,
    title = "{M}ulti{S}kill: Evaluating Large Multimodal Models for Fine-grained Alignment Skills",
    author = "Xu, Zhenran and Shi, Senbao and Hu, Baotian and Wang, Longyue and Zhang, Min",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    year = "2024",
    pages = "1506--1523"
}
（这个论文已经发表了，他的方法和框架可以借鉴，比如他说的目前的评估体系的问题，和他们的评估框架的方法）


(d) Zhang et al. (2024): LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models
提出了LMMs-Eval，一个统一的多模态模型评估套件，包含50多个任务和10多个模型，旨在促进透明且可复现的评估。文章还提出了LMMs-Eval Lite和LiveBench，前者强调覆盖面和效率，后者利用不断更新的新闻和在线论坛评估模型在野外的泛化能力。研究强调了在评估大型多模态模型时考虑评估三难困境(wide-coverage、low-cost和zero-contamination)的重要性。
（可以用于引用的，评估框架部分的文章）
@inproceedings{zhang-etal-2025-lmms,
    title = "{LMM}s-Eval: Reality Check on the Evaluation of Large Multimodal Models",
    author = "Zhang, Kaichen  and
      Li, Bo  and
      Zhang, Peiyuan  and
      Pu, Fanyi  and
      Cahyono, Joshua Adrian  and
      Hu, Kairui  and
      Liu, Shuai  and
      Zhang, Yuanhan  and
      Yang, Jingkang  and
      Li, Chunyuan  and
      Liu, Ziwei",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2025",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-naacl.51/",
    pages = "881--916",
    ISBN = "979-8-89176-195-7",
    abstract = "The advances of large foundation models necessitate wide-coverage, low-cost, and zero-contamination benchmarks. Despite continuous exploration of language model evaluations, comprehensive studies on the evaluation of Large Multi-modal Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified and standardized multimodal benchmark framework with over 50 tasks and more than 10 models to promote transparent and reproducible evaluations. Although LMMS-EVAL offers comprehensive coverage, we find it still falls short in achieving low cost and zero contamination. To approach this evaluation trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that emphasizes both coverage and efficiency. Additionally, we present Multimodal LIVEBENCH that utilizes continuously updating news and online forums to assess models' generalization abilities in the wild, featuring a low-cost and zero-contamination evaluation approach. In summary, our work highlights the importance of considering the evaluation trilemma and provides practical solutions to navigate the trade-offs in evaluating large multi-modal models, paving the way for more effective and reliable benchmarking of LMMs."
}


3. Chen et al. (2024a): GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI
这是一个医学领域的研究成果，也就是大语言模型和多模态数据处理，提供一个医疗领域的辅助诊断支持系统。使用gpt4o，但是运行结果不好。（这个内容可以用于，我的论文的平行的NLP语言处理模型的应用，说明最近的研究有多广泛。）
@inproceedings{
    chen2024gmaimmbench,
    title={{GMAI}-{MMB}ench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical {AI}},
    author={pengcheng chen and Jin Ye and Guoan Wang and Yanjun Li and Zhongying Deng and Wei Li and Tianbin Li and Haodong Duan and Ziyan Huang and Yanzhou Su and Benyou Wang and Shaoting Zhang and Bin Fu and Jianfei Cai and Bohan Zhuang and Eric J Seibel and Junjun He and Yu Qiao},
    booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    year={2024},
    url={https://openreview.net/forum?id=K6b8LCXBeQ}
    }

4. Fu et al. (2024): MME: A comprehensive evaluation benchmark for multimodal large language models
@misc{fu2024mmecomprehensiveevaluationbenchmark,
    title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models}, 
    author={Chaoyou Fu and Peixian Chen and Yunhang Shen and Yulei Qin and Mengdan Zhang and Xu Lin and Jinrui Yang and Xiawu Zheng and Ke Li and Xing Sun and Yunsheng Wu and Rongrong Ji},
    year={2024},
    eprint={2306.13394},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2306.13394}, 
}
（可以用于横向对比，说明评估框架在不断被研发）


@inproceedings{Yuan2023ArtGPT4TA,
  title={ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter},
  author={Zheng Yuan and HU Xue and Xinyi Wang and Yongming Liu and Zhuanzhe Zhao and Kun Wang},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258676618}
}
这个很有用，是一个艺术评论gpt进行了结构设计和微调，强化学习这些内容，但是他并没有提出一个合理的自动化的量化评估框架，来证明其评论的质量。这导致难以长久可持续的发展。
## 研究概述

评估多模态大型语言模型(MLLMs)对文化内容的理解面临独特挑战。近期研究开发了专门的多模态评估基准，但这些基准在文化领域往往缺乏深度。关于MLLMs艺术解释能力的研究已识别出跨文化理解和符号解释方面的局限性。本研究通过开发专门针对中国艺术评论的结构化评估框架来解决这些差距，结合向量空间分析和特定能力评估，为MLLMs在这一文化丰富领域的表现提供更全面的评估。 