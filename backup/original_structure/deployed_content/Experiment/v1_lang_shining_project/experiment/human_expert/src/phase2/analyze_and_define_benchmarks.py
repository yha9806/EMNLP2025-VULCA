"""
Script for Phase 1.1: Analyze Human Expert Features and Define Benchmark Profiles.

This script loads the processed features from human expert critiques (output of 
`extract_human_expert_features.py`) and performs exploratory data analysis (EDA).
The goal is to provide insights that will help the researcher define and quantify
the four "Critique Profile" benchmarks:
1. Historical Research Focused (博古通今型)
2. Aesthetic Appreciation Focused (尚意唯美型)
3. Critical & Theoretical Focused (思辨求新型)
4. Balanced & Comprehensive (综合平衡型)

The script itself does not *create* the final benchmark model (which is a set of 
defined criteria), but rather provides the analytical tools and a framework to 
support the researcher in establishing those criteria based on data and domain expertise.
"""
import os
import pandas as pd
import ast  # For safely evaluating string representations of dictionaries
import numpy as np
from collections import Counter

# Optional: for visualizations - uncomment if you want to generate plots
import matplotlib.pyplot as plt
import seaborn as sns

# Optional: Configure Matplotlib for Chinese font display
# Ensure you have a Chinese font installed (e.g., SimHei, Microsoft YaHei)
# plt.rcParams['font.sans-serif'] = ['SimHei'] # Example: Replace with your font
# plt.rcParams['axes.unicode_minus'] = False # Display negative signs correctly

# --- Configuration ---
# Path to the consolidated CSV file generated by extract_human_expert_features.py
CONSOLIDATED_CSV_PATH = "../result/human_expert_features_consolidated.csv"

# Directory for saving generated EDA plots
PLOTS_OUTPUT_DIR = "../result/eda_plots/"

# Directory for saving analysis result CSV files
ANALYSIS_OUTPUT_DIR = "../result/analysis_results/"

# --- Helper Function: Load and Preprocess Data ---
def load_and_preprocess_data(csv_path):
    """Loads the CSV data and preprocesses dictionary-like columns."""
    if not os.path.exists(csv_path):
        print(f"Error: Consolidated CSV file not found at {csv_path}")
        print("Please ensure `extract_human_expert_features.py` has been run successfully.")
        return None
    
    print(f"Loading data from {csv_path}...")
    df = pd.read_csv(csv_path)
    print(f"Loaded {len(df)} critiques.")

    # Convert stringified dictionaries back to dict objects
    # Handle potential errors if a cell is not a valid dict string (e.g., NaN or malformed)
    for col_name in ['features', 'quality']:
        try:
            df[col_name] = df[col_name].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) and isinstance(x, str) else None)
        except Exception as e:
            print(f"Warning: Could not parse column '{col_name}'. Error: {e}. Ensure it contains valid dictionary strings or NaN.")
            df[col_name] = None # Or some other error handling
    
    print("Data preprocessing (dictionary conversion) complete.")
    return df

# --- Helper Function: Analyze Dictionary-based Label Columns (Features/Quality) ---
def get_label_stats_from_dict_column(df, column_name, profile_name="Overall"):
    """ 
    Analyzes columns like 'features' or 'quality' where each cell contains a dictionary 
    of {label: {'score': float, 'zh': str}}.
    Calculates mention frequency and average score for each label.
    """
    print(f"\n--- Analyzing '{column_name}' for '{profile_name}' profile --- ")
    if column_name not in df.columns or df[column_name].isnull().all():
        print(f"Column '{column_name}' is missing or all values are null. Skipping analysis.")
        return None

    all_labels_scores = {}
    label_mentions = Counter()
    total_valid_critiques = 0

    for _, row in df.iterrows():
        dict_data = row[column_name]
        if isinstance(dict_data, dict):
            total_valid_critiques += 1
            for label_en, details in dict_data.items():
                if isinstance(details, dict) and 'score' in details:
                    label_mentions[label_en] += 1
                    if label_en not in all_labels_scores:
                        all_labels_scores[label_en] = []
                    all_labels_scores[label_en].append(details['score'])
    
    if total_valid_critiques == 0:
        print(f"No valid dictionary data found in column '{column_name}' for analysis.")
        return {}

    label_analysis = {}
    print(f"Based on {total_valid_critiques} critiques with valid '{column_name}' data:")
    for label, scores in all_labels_scores.items():
        mention_rate = (label_mentions[label] / total_valid_critiques) * 100
        avg_score = np.mean(scores) if scores else 0
        std_score = np.std(scores) if scores else 0
        # Assuming the first zh translation encountered is representative
        # This could be improved if zh translations vary, but they shouldn't for a given EN label
        first_zh = ""
        for _, row_inner in df.iterrows(): 
            dict_data_inner = row_inner[column_name]
            if isinstance(dict_data_inner, dict) and label in dict_data_inner and 'zh' in dict_data_inner[label]:
                first_zh = dict_data_inner[label]['zh']
                break
        
        label_analysis[label] = {
            'mention_count': label_mentions[label],
            'mention_rate_percent': round(mention_rate, 2),
            'avg_score': round(avg_score, 3),
            'std_dev_score': round(std_score, 3),
            'min_score': round(np.min(scores), 3) if scores else 0,
            'max_score': round(np.max(scores), 3) if scores else 0,
            'zh_translation': first_zh
        }
        print(f"  Label: {label} ({first_zh}) - Mention Rate: {mention_rate:.2f}%, Avg Score: {avg_score:.3f}")
    
    return label_analysis

# --- Helper Function: Plot Label Statistics (Mention Rate & Avg Score) ---
def plot_label_stats(stats_dict, title_prefix, output_dir):
    """Generates and saves bar plots for mention rate and average score from stats dict."""
    if not stats_dict:
        print(f"No stats provided for {title_prefix}. Skipping plots.")
        return

    # Convert stats dict to DataFrame for easier plotting
    df_stats = pd.DataFrame.from_dict(stats_dict, orient='index')
    # Use Chinese translation as index for better readability in plots if needed, otherwise use English label
    # df_stats.index = df_stats['zh_translation'] 
    df_stats = df_stats.sort_values(by='mention_rate_percent', ascending=False)

    plt.figure(figsize=(12, len(df_stats) * 0.4)) # Adjust figure size based on number of labels
    sns.barplot(x='mention_rate_percent', y=df_stats.index, data=df_stats, palette="viridis")
    plt.title(f'{title_prefix} - Mention Rate (%)')
    plt.xlabel("Mention Rate (%)")
    plt.ylabel("Label")
    plt.tight_layout()
    plot_filename_mention = os.path.join(output_dir, f"{title_prefix.lower().replace(' ', '_')}_mention_rate.png")
    try:
        plt.savefig(plot_filename_mention)
        print(f"Saved mention rate plot: {plot_filename_mention}")
    except Exception as e:
        print(f"Error saving plot {plot_filename_mention}: {e}")
    plt.close()

    df_stats = df_stats.sort_values(by='avg_score', ascending=False)
    plt.figure(figsize=(12, len(df_stats) * 0.4))
    sns.barplot(x='avg_score', y=df_stats.index, data=df_stats, palette="magma")
    plt.title(f'{title_prefix} - Average Score')
    plt.xlabel("Average Score")
    plt.ylabel("Label")
    plt.tight_layout()
    plot_filename_score = os.path.join(output_dir, f"{title_prefix.lower().replace(' ', '_')}_average_score.png")
    try:
        plt.savefig(plot_filename_score)
        print(f"Saved average score plot: {plot_filename_score}")
    except Exception as e:
        print(f"Error saving plot {plot_filename_score}: {e}")
    plt.close()

# --- Main Execution: Exploratory Data Analysis & Framework for Profile Definition ---
if __name__ == "__main__":
    # Create output directories if they don't exist
    if PLOTS_OUTPUT_DIR and not os.path.exists(PLOTS_OUTPUT_DIR):
        os.makedirs(PLOTS_OUTPUT_DIR)
        print(f"Created plots output directory: {PLOTS_OUTPUT_DIR}")
    if ANALYSIS_OUTPUT_DIR and not os.path.exists(ANALYSIS_OUTPUT_DIR):
        os.makedirs(ANALYSIS_OUTPUT_DIR)
        print(f"Created analysis results output directory: {ANALYSIS_OUTPUT_DIR}")

    df_critiques = load_and_preprocess_data(CONSOLIDATED_CSV_PATH)

    if df_critiques is not None:
        print("\n=== Exploratory Data Analysis (EDA) for Human Expert Critiques ===")

        # A. Stance Analysis
        print("\n--- Analyzing Stance --- ")
        if 'stance_label_en' in df_critiques.columns:
            print("Distribution of Stance Labels (English):")
            stance_dist = df_critiques['stance_label_en'].value_counts(normalize=True) * 100
            print(stance_dist)
            # Save Stance Distribution
            try:
                stance_dist_df = stance_dist.reset_index()
                stance_dist_df.columns = ['stance_label_en', 'percentage']
                stance_dist_path = os.path.join(ANALYSIS_OUTPUT_DIR, "stance_distribution.csv")
                stance_dist_df.to_csv(stance_dist_path, index=False, encoding='utf-8-sig')
                print(f"Saved stance distribution data: {stance_dist_path}")
            except Exception as e:
                print(f"Error saving stance distribution CSV: {e}")
            
            # Plot Stance Distribution
            plt.figure(figsize=(10, 6))
            sns.countplot(y=df_critiques['stance_label_en'], order=df_critiques['stance_label_en'].value_counts().index, palette='Spectral')
            plt.title('Distribution of Primary Stance Labels')
            plt.xlabel('Count')
            plt.ylabel('Stance Label')
            plt.tight_layout()
            plot_filename_stance_dist = os.path.join(PLOTS_OUTPUT_DIR, "stance_distribution.png")
            try:
                plt.savefig(plot_filename_stance_dist)
                print(f"Saved stance distribution plot: {plot_filename_stance_dist}")
            except Exception as e:
                print(f"Error saving plot {plot_filename_stance_dist}: {e}")
            plt.close()

            print("\nDescriptive Statistics for Stance Score:")
            stance_stats = df_critiques.groupby('stance_label_en')['stance_score'].agg(['mean', 'std', 'min', 'max', 'count'])
            print(stance_stats)
            # Save Stance Score Stats
            try:
                stance_stats_path = os.path.join(ANALYSIS_OUTPUT_DIR, "stance_score_stats.csv")
                stance_stats.reset_index().to_csv(stance_stats_path, index=False, encoding='utf-8-sig')
                print(f"Saved stance score stats data: {stance_stats_path}")
            except Exception as e:
                print(f"Error saving stance score stats CSV: {e}")
            
            # Plot Stance Score Distribution
            plt.figure(figsize=(12, 7))
            # Only plot for stances with a reasonable number of samples, e.g., > 1? 
            # Or sort by count? Let's sort by count for better viz
            order = df_critiques['stance_label_en'].value_counts().index
            sns.boxplot(x='stance_score', y='stance_label_en', data=df_critiques, order=order, palette='coolwarm')
            plt.title('Distribution of Stance Scores by Label')
            plt.xlabel('Stance Score')
            plt.ylabel('Stance Label')
            plt.tight_layout()
            plot_filename_stance_score = os.path.join(PLOTS_OUTPUT_DIR, "stance_score_distribution.png")
            try:
                plt.savefig(plot_filename_stance_score)
                print(f"Saved stance score distribution plot: {plot_filename_stance_score}")
            except Exception as e:
                print(f"Error saving plot {plot_filename_stance_score}: {e}")
            plt.close()
        else:
            print("Stance columns ('stance_label_en', 'stance_score') not found.")

        # B. Features (Core Focal Points) Analysis
        features_stats = get_label_stats_from_dict_column(df_critiques, 'features')
        if features_stats:
            # Save Features Stats
            try:
                features_stats_df = pd.DataFrame.from_dict(features_stats, orient='index')
                features_stats_path = os.path.join(ANALYSIS_OUTPUT_DIR, "features_analysis.csv")
                features_stats_df.reset_index().rename(columns={'index': 'feature_label_en'}).to_csv(features_stats_path, index=False, encoding='utf-8-sig')
                print(f"Saved features analysis data: {features_stats_path}")
            except Exception as e:
                print(f"Error saving features analysis CSV: {e}")
            # Plotting call remains
            plot_label_stats(features_stats, "Core Focal Points", PLOTS_OUTPUT_DIR)

        # C. Quality (Argumentative Quality) Analysis
        quality_stats = get_label_stats_from_dict_column(df_critiques, 'quality')
        if quality_stats:
             # Save Quality Stats
            try:
                quality_stats_df = pd.DataFrame.from_dict(quality_stats, orient='index')
                quality_stats_path = os.path.join(ANALYSIS_OUTPUT_DIR, "quality_analysis.csv")
                quality_stats_df.reset_index().rename(columns={'index': 'quality_label_en'}).to_csv(quality_stats_path, index=False, encoding='utf-8-sig')
                print(f"Saved quality analysis data: {quality_stats_path}")
            except Exception as e:
                print(f"Error saving quality analysis CSV: {e}")
            # Plotting call remains
            plot_label_stats(quality_stats, "Argumentative Quality", PLOTS_OUTPUT_DIR)

        print("\n\n=== Framework for Defining Critique Profile Benchmarks ===")
        print("The following sections provide a template for defining quantitative criteria for each critique profile.")
        print("You (the researcher) should fill in the `thresholds` based on the EDA results above and your domain expertise.")
        print("The `check_profile_*` functions are examples of how you might apply these criteria.")

        # You would typically define these label maps based on the ones in extract_human_expert_features.py
        # For brevity, they are assumed to be known when interpreting results.

        # --- Profile 1: Historical Research Focused (博古通今型) ---
        def check_profile_historical_research(critique_row, thresholds):
            match_score = 0
            conditions_met = 0
            total_conditions = 0

            # Example stance check
            if 'stance_Historical Research_min_score' in thresholds and \
               critique_row['stance_label_en'] == 'Historical Research' and \
               critique_row['stance_score'] >= thresholds['stance_Historical Research_min_score']:
                match_score += critique_row['stance_score'] # or a fixed value
                conditions_met +=1
            total_conditions +=1
            
            # Example feature check (presence and score)
            features_dict = critique_row['features']
            if isinstance(features_dict, dict) and 'feature_Historical Context_min_score' in thresholds and \
               'Historical Context' in features_dict and \
               features_dict['Historical Context']['score'] >= thresholds['feature_Historical Context_min_score']:
                match_score += features_dict['Historical Context']['score']
                conditions_met +=1
            total_conditions +=1

            if isinstance(features_dict, dict) and 'feature_Technique Inheritance & Innovation_min_score' in thresholds and \
               'Technique Inheritance & Innovation' in features_dict and \
               features_dict['Technique Inheritance & Innovation']['score'] >= thresholds['feature_Technique Inheritance & Innovation_min_score']:
                match_score += features_dict['Technique Inheritance & Innovation']['score']
                conditions_met +=1
            total_conditions +=1
            
            # Example quality check
            quality_dict = critique_row['quality']
            if isinstance(quality_dict, dict) and 'quality_Classical Citations_min_score' in thresholds and \
               'Classical Citations' in quality_dict and \
               quality_dict['Classical Citations']['score'] >= thresholds['quality_Classical Citations_min_score']:
                match_score += quality_dict['Classical Citations']['score']
                conditions_met +=1
            total_conditions +=1
            
            # Return True if a certain number of conditions are met, or based on match_score
            # This is a simplified example; your logic can be more complex.
            return conditions_met >= thresholds.get("min_conditions_to_match", 2) # e.g. match if at least 2 conditions are met

        # --- Profile 2: Aesthetic Appreciation Focused (尚意唯美型) ---
        def check_profile_aesthetic_appreciation(critique_row, thresholds):
            # Similar logic as above, but with different labels and thresholds
            # e.g., stance_Aesthetic Appreciation_min_score, feature_Artistic Conception_min_score, etc.
            pass

        # --- Profile 3: Critical & Theoretical Focused (思辨求新型) ---
        def check_profile_critical_theoretical(critique_row, thresholds):
            # e.g., stance_Theoretical Construction_min_score, stance_Critical Inquiry_min_score, 
            # quality_Profound Insight_min_score, quality_Clear Logic_min_score, etc.
            pass

        # --- Profile 4: Balanced & Comprehensive (综合平衡型) ---
        def check_profile_balanced_comprehensive(critique_row, thresholds):
            # This might involve checking for moderate scores across a wider range of labels
            # or ensuring no single stance/feature сильно dominates, and good overall quality scores.
            pass

        print("\nExample: Applying a hypothetical profile check (after defining thresholds)")
        # 1. DEFINE YOUR THRESHOLDS BASED ON EDA AND EXPERTISE
        #    These are illustrative and NEED to be refined by the researcher!
        historical_research_thresholds = {
            "stance_Historical Research_min_score": 0.6, # Stance should be Historical Research with score >= 0.6
            "feature_Historical Context_min_score": 0.5, # Feature 'Historical Context' should have score >= 0.5
            "feature_Technique Inheritance & Innovation_min_score": 0.0, # No minimum for this one in this example
            "quality_Classical Citations_min_score": 0.4,    # Quality 'Classical Citations' should have score >= 0.4
            "min_conditions_to_match": 2 # Let's say at least 2 of these conditions must be met
        }

        # 2. APPLY TO THE DATAFRAME (this is just an example for a few rows)
        # df_critiques['is_historical_research_profile'] = df_critiques.apply(
        #     lambda row: check_profile_historical_research(row, historical_research_thresholds), axis=1
        # )
        # print("\nNumber of critiques matching 'Historical Research Profile' (example thresholds):")
        # print(df_critiques['is_historical_research_profile'].value_counts())

        print("\nNext Steps as per README Phase 1.1:")
        print("1. Deeply analyze the EDA results printed above.")
        print("2. Refine the qualitative descriptions for each of the four profiles.")
        print("3. Based on EDA and your expertise, define concrete quantitative thresholds for the key features of each profile (like the `historical_research_thresholds` example). ")
        print("4. Implement or refine the `check_profile_*` functions with your defined thresholds and logic.")
        print("5. Apply these functions to your dataset to see how human expert texts align with these profiles.")
        print("6. Document these profiles and their quantitative criteria – this forms your benchmark model.")

    else:
        print("Could not load data. Analysis cannot proceed.")
    
    print("\nScript finished.") 